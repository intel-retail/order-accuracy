# =============================================================================
# Order Accuracy Take-Away Configuration
# =============================================================================
# Copy this file to .env and modify as needed:
#   cp .env.example .env
#
# Required variables are marked with [REQUIRED]
# Optional variables have default values shown
# =============================================================================

# -----------------------------------------------------------------------------
# Service Mode Configuration
# -----------------------------------------------------------------------------
# SERVICE_MODE: 'single' (UI-only) or 'parallel' (background workers)
SERVICE_MODE=single

# WORKERS: Number of station workers (0 = no background processing)
WORKERS=0

# SCALING_MODE: 'fixed' or 'auto' for parallel mode
SCALING_MODE=fixed

# -----------------------------------------------------------------------------
# VLM Backend Configuration (Order Accuracy Service)
# -----------------------------------------------------------------------------
# VLM_BACKEND: 'ovms' (recommended) or 'openvino' (local)
VLM_BACKEND=ovms

# OVMS connection settings
OVMS_ENDPOINT=http://ovms-vlm:8000
OVMS_MODEL_NAME=Qwen/Qwen2.5-VL-7B-Instruct-ov-int8

# OpenVINO local settings (when VLM_BACKEND=openvino)
OPENVINO_DEVICE=GPU
VLM_MODEL_PATH=/model/Qwen2.5-VL-7B-Instruct-ov-int8

# -----------------------------------------------------------------------------
# Semantic Search Agent Configuration
# -----------------------------------------------------------------------------
# SEMANTIC_VLM_BACKEND: 'ovms' or 'openvino' for semantic service
SEMANTIC_VLM_BACKEND=ovms

# DEFAULT_MATCHING_STRATEGY: 'exact', 'semantic', or 'hybrid'
DEFAULT_MATCHING_STRATEGY=hybrid

# SIMILARITY_THRESHOLD: Fuzzy matching threshold (0.0 to 1.0)
SIMILARITY_THRESHOLD=0.85

# OVMS_TIMEOUT: Timeout for VLM requests in seconds
OVMS_TIMEOUT=60

# SEMANTIC_LOG_LEVEL: Logging level (DEBUG, INFO, WARNING, ERROR)
SEMANTIC_LOG_LEVEL=INFO

# Cache settings
CACHE_TTL=3600

# OpenVINO model path for semantic service (when SEMANTIC_VLM_BACKEND=openvino)
OPENVINO_MODEL_PATH=/models/Qwen2.5-VL-7B-Instruct-ov-int8

# -----------------------------------------------------------------------------
# MinIO Object Storage
# -----------------------------------------------------------------------------
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin

# -----------------------------------------------------------------------------
# Port Configuration
# -----------------------------------------------------------------------------
# API_PORT: Order accuracy API port
API_PORT=8000

# GRADIO_PORT: Gradio UI port
GRADIO_PORT=7860

# OVMS_PORT: OVMS external port
OVMS_PORT=8001

# SEMANTIC_API_PORT: Semantic service API port
SEMANTIC_API_PORT=8080

# SEMANTIC_METRICS_PORT: Semantic service metrics port
SEMANTIC_METRICS_PORT=9090

# -----------------------------------------------------------------------------
# Performance Tuning
# -----------------------------------------------------------------------------
OMP_NUM_THREADS=4
MKL_NUM_THREADS=4
OPENBLAS_NUM_THREADS=4

# -----------------------------------------------------------------------------
# Proxy Settings (Intel internal network)
# -----------------------------------------------------------------------------
HTTP_PROXY=http://proxy-pilot.intel.com:912
HTTPS_PROXY=http://proxy-pilot.intel.com:912
NO_PROXY=localhost,127.0.0.1,order-accuracy,minio,ovms-vlm,semantic-service,rtsp-streamer,host.docker.internal

# -----------------------------------------------------------------------------
# RTSP Streaming Configuration (Parallel Mode)
# -----------------------------------------------------------------------------
# RTSP_STREAM_NAME: Source video file name (without .mp4 extension)
# This video file will be streamed to all stations
RTSP_STREAM_NAME=384-651-925

# RTSP_STREAMS: Custom stream names (comma-separated, optional)
# Leave empty to auto-generate station_1, station_2, etc. (one per WORKER)
# When set, these custom names are used instead of station_N
# Example: RTSP_STREAMS=checkout_1,checkout_2,checkout_3
RTSP_STREAMS=

# -----------------------------------------------------------------------------
# Benchmarking Configuration (Stream Density Testing)
# -----------------------------------------------------------------------------
# These variables configure the stream density latency benchmark script.
# CLI arguments override these environment variables.

# TARGET_LATENCY_MS: Target latency threshold in milliseconds
# Benchmark scales workers until this latency is exceeded
TARGET_LATENCY_MS=15000

# LATENCY_METRIC: Which latency metric to use for scaling decisions
# Options: 'avg' (average) or 'p95' (95th percentile)
LATENCY_METRIC=avg

# WORKER_INCREMENT: Number of workers to add per iteration
# Higher values = faster benchmark but less granular results
WORKER_INCREMENT=1

# INIT_DURATION: Initialization/warmup time in seconds per iteration
# Time to wait after starting containers before collecting metrics
INIT_DURATION=120

# MIN_TRANSACTIONS: Minimum transactions required before measuring latency
# Lower values = faster iterations but less reliable metrics
MIN_TRANSACTIONS=3

# MAX_ITERATIONS: Maximum number of scaling iterations
# Safety limit to prevent runaway tests
MAX_ITERATIONS=50

# MAX_WAIT_SEC: Maximum wait time per iteration in seconds
# Timeout if MIN_TRANSACTIONS not reached within this time
MAX_WAIT_SEC=600

# RESULTS_DIR: Directory for benchmark results output
RESULTS_DIR=./results
